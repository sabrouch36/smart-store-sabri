
> Use this repo to start a professional Python BI project that reads raw data into pandas DataFrames.

- Course: Business Intelligence
- Instructor: Dr. Denise Case
- Repository Template: <https://github.com/denisecase/pro-analytics-02>
- Author: **Sabri Hamdaoui**, MBA â€“ Data Analytics
- University: Northwest Missouri State University
- GitHub: [sabrouch36](https://github.com/sabrouch36)

---

# Smart Store Data Preparation (P3)

This project prepares raw business data (Customers, Products, and Sales) so it is clean, consistent, and ready for use in analytics and future ETL processes.

## Project Goal
The goal of this phase is to clean and standardize datasets to ensure accuracy and reliability when performing reporting, visualizations, or loading into a central data warehouse.

## Key Tasks Performed
- Removed duplicate records
- Handled missing values using simple fill strategies
- Standardized text fields (trim + lowercase)
- Automatically detected correct name columns
- Removed numeric outliers where appropriate
- Saved cleaned outputs into a `prepared/` data directory

## Files Processed
| Raw Dataset               | Output File                      |
|--------------------------|----------------------------------|
| `customers_data.csv`     | `customers_prepared.csv`         |
| `products_data.csv`      | `products_prepared.csv`          |
| `sales_data.csv`         | `sales_prepared.csv`             |

## Main Script
The primary pipeline is located at:
src/analytics_project/data_prep.py


## The reusable data cleaning helper class is located at:
src/utils/data_scrubber.py


## How to Run
From the project root:

uv sync
uv run python -m analytics_project.data_prep


## Output Location

Cleaned data is stored here:
/data/prepared/

## Logging

All execution logs are stored in:
project.log


Data is now clean and ready for the ETL and analysis steps in upcoming modules.
---

## ğŸ“˜ P4 â€“ Create and Populate Data Warehouse (DW)

### ğŸ¯ Project Overview
In this phase, we designed and implemented a **data warehouse** using SQLite to support business intelligence and analytics.
The project follows the workflow: **Design â†’ Build â†’ Load â†’ Verify â†’ Document**.

### ğŸ§± Schema Design
The warehouse uses a **Star Schema** consisting of:
- **Fact Table:** `sale`
- **Dimension Tables:** `customer`, `product`

Each sale record references a customer and a product, enabling cross-analysis of performance and customer behavior.

**Entity Relationships**
- `sale.customer_id` â†’ `customer.customer_id`
- `sale.product_id` â†’ `product.product_id`

### âš™ï¸ Implementation
The ETL script [`etl_to_dw.py`](src/analytics_project/etl_to_dw.py):
- Creates the SQLite database `smart_sales.db` in `data/dw/`
- Defines tables using SQL `CREATE TABLE` statements
- Loads prepared CSV files from `data/prepared/`
- Removes duplicates, enforces primary and foreign keys
- Commits all data into the warehouse and validates relationships

### ğŸ“Š Verification
After running the script, we verified:
| Table | Row Count | Status |
|--------|------------|---------|
| customer | 200 | âœ… |
| product | 100 | âœ… |
| sale | 1999 | âœ… |

**Foreign Keys:** OK âœ…
**Null / Duplicate Checks:** All Passed âœ…

### ğŸ§ª Example Query (Join)
```sql
SELECT s.sale_id, c.name, p.product_name, s.sale_amount, s.sale_date
FROM sale s
JOIN customer c USING (customer_id)
JOIN product  p USING (product_id)
LIMIT 10;

ğŸ§­ How to Run
python src/analytics_project/etl_to_dw.py
This will recreate the DW schema and reload all prepared data.
```
## ğŸ“¸ Screenshots

![Customer Table](docs/images/customer-sample.png)
![Sale Table](docs/images/sale-sample.png)
![Row Counts](docs/images/dw-rowcounts.png)
![FK Check](docs/images/dw-fkcheck.png)

ğŸ“Š P5 â€“ Cross-Platform Reporting with Power BI
ğŸ¯ Overview

In this phase, we used Power BI Desktop (Windows implementation) to connect to the SQLite data warehouse created in P4, perform OLAP operations, and generate interactive business intelligence reports.

The reporting workflow included:

Connecting Power BI to the SQLite warehouse via ODBC

Importing the customer, product, and sale tables

Cleaning the date column and converting it to a proper Date type

Creating slicers, pivot tables, and charts

Applying slice, dice, and drilldown operations

Producing final visualizations to analyze sales performance by category and region

ğŸ—‚ï¸ Data Model in Power BI

After loading the DW, the following model appeared in Power BI:

sale (fact table)

customer (dimension)

product (dimension)

Relationships were based on:

sale.customer_id â†’ customer.customer_id

sale.product_id â†’ product.product_id

âœ” Relationships created successfully
âœ” Date column cleaned and converted from text to Date
âœ” Error row removed (2023-13-01)

ğŸ” OLAP Operations
1ï¸âƒ£ Slice (Filter by Date)

Using a date slicer, sales were filtered using a specific date range:

Added a Slicer visual

Placed sale_date in the slicer

Set slicer type to Between

Result: shows sales only for the chosen time window
### ğŸ“Œ Slice Operation (Date Filter)

![Slice](docs/images/slice.png)


2ï¸âƒ£ Dice (Category Ã— Region Pivot)

Created a matrix table to analyze sales by:

Rows: product category

Columns: customer region

Values: Sum of sale_amount

This produced a full cross-tab analysis (category Ã— region), including totals.

ğŸ“¸ Matching screenshot:

### ğŸ² Dice Operation (Category Ã— Region)

![Dice](docs/images/dice.png)


3ï¸âƒ£ Drilldown (Chart Visualization)

A clustered column chart was created:

X-axis: region

Legend: category

Y-axis: Sum of sale_amount

This enables:

Visual drilldown by clicking a category

Comparing regions

Seeing category patterns clearly

ğŸ“¸ Matching screenshot:

![drilldown](docs/images/drilldown.png)

ğŸ“ˆ Final Visuals Included

âœ” Slicer for sale_date

âœ” Pivot (matrix) for category Ã— region

âœ” Clustered column chart (drilldown enabled)

âœ” Cleaned and validated data model

ğŸ› ï¸ Tools Used

Power BI Desktop (Windows)

ODBC connection to SQLite (smart_sales.db)

Star schema (fact_sale + dimensions)

ğŸ“„ How to Reproduce

Launch Power BI Desktop

Get Data â†’ ODBC â†’ Select SQLite connection

Load the three DW tables

Convert sale_date column to Date

Create slicer, matrix, and chart

Save report as .pbix

Export screenshots for documentation

ğŸ“¸ Required Submission Images (all completed)


âœ” Slice result (date filter)

âœ” Dice result (category Ã— region matrix)

âœ” Drilldown chart (clustered columns)


ğŸ“˜ P6 â€“ BI Insights and Storytelling (OLAP with Python)
ğŸ¯ Project Goal

The goal of this phase is to apply OLAP slicing, dicing, and drilldown techniques using Python to analyze seasonal sales trends and uncover actionable business insights.

Business Objective Selected:
Identify seasonal sales patterns across product categories and regions to improve inventory planning.

Short Name: Seasonal Sales Trends

This analysis helps the business prepare sufficient stock for high-demand periods, reduce overstock in slow months, and align staffing with sales peaks.

ğŸ—‚ï¸ Section 1 â€” Data Source

For this project, I used the data warehouse (smart_sales.db) created in P4.
The warehouse contains:

Table	Description
sale	Fact table of all transactions
customer	Customer dimension
product	Product dimension

Columns used:

From sale

sale_id

customer_id

product_id

sale_amount

sale_date

From customer

region

From product

category

These were combined into a single dataset using Python JOIN operations.

ğŸ§° Section 2 â€” Tools Used

Python (Pandas, Matplotlib, Seaborn)

SQLite (warehouse)

Jupyter/VS Code terminal

uv package manager

Python was chosen because it supports:

automated OLAP-style analysis

fast grouping/aggregation

repeatable and scalable workflows

ğŸ” Section 3 â€” Workflow & OLAP Logic
âœ” Step 1 â€” Load DW Tables

Loaded sale, customer, and product tables from the SQLite warehouse into Pandas DataFrames.

âœ” Step 2 â€” Join Tables (DW â†’ Analysis DataFrame)

Performed two JOIN operations:

sale Ã— customer on customer_id

sale Ã— product on product_id

Result: a fully enriched dataset with region + category info per transaction.

âœ” Step 3 â€” Clean & Transform

Converted sale_date to proper datetime

Fixed invalid dates using errors="coerce"

Extracted:

year

month

Selected the useful OLAP fields

Final clean fields:

| year | month | category | region | sale_amount | sale_id |

ğŸ§® Section 4 â€” Build OLAP Cube

Using groupby we generated a multi-dimensional cube:

Dimensions:

year

month

category

region

Metrics:

total_sales = SUM(sale_amount)

transactions = COUNT(sale_id)

AOV = total_sales / transactions

Example (first rows):

year	month	category	region	total_sales	AOV

Cube shape: (32, 7)

ğŸªœ Section 5 â€” OLAP Operations

1ï¸âƒ£ Slice (Filter a single dimension)
slice_2025 = cube[cube["year"] == 2025]
âœ” Shows only sales for year 2025.

2ï¸âƒ£ Dice (Filter multiple dimensions)
cube[(cube["category"] == "clothing") & (cube["region"].str.lower() == "north")]
âœ” Clothing sales in the North region.

3ï¸âƒ£ Drilldown (Year â†’ Month)

Aggregated yearly totals:
Then drilled down to monthly totals:
yearly = cube.groupby("year").sum()
monthly = cube.groupby(["year", "month"]).sum()

âœ” Shows high-level â†’ detailed trend navigation.

ğŸ“Š Section 6 â€” Visualizations
ğŸ“ˆ 1) Line Chart â€“ Seasonal Trends by Category

Shows monthly sales patterns:
![Seasonal Trends](docs/images/line_trends.png)

Interpretation:

Product categories show variation in seasonal demand.

Home and Electronics showed higher total sales in the provided dataset.

ğŸ”¥ 2) Heatmap (Month Ã— Category)

![Heatmap](docs/images/heatmap_sales.png)

Interpretation:

Month 5 showed highest activity across all categories.

Heatmap highlights which product categories spike during specific months.

ğŸ“Š 3) Bar Chart â€“ Total Sales by Region

![Region Sales](docs/images/region_sales.png)

Interpretation:

North and West regions generated the highest revenue.

Useful for regional stocking and staffing decisions.

ğŸ§­ Section 7 â€” Business Insights

âœ” The North and West regions consistently outperform others
âœ” Electronics and Home categories generate the largest revenue
âœ” Month 5 shows a strong seasonal spike
âœ” AOV is highest in high-sales categories (e.g., Home)

ğŸ› ï¸ Section 8 â€” Recommended Business Actions

Based on the OLAP analysis:

ğŸ“Œ Inventory

Increase inventory for Home and Electronics categories before peak season.

Reduce overstock for lower-performing regions.

ğŸ“Œ Staffing

Add temporary staff in North + West regions during high-demand months.

ğŸ“Œ Marketing

Launch targeted promotions for categories with moderate performance.

ğŸ“Œ Regional Strategy

Investigate reasons behind lower sales in â€œsouth-westâ€ and â€œeastâ€.

âš ï¸ Section 9 â€” Challenges Encountered

Invalid dates (2023-13-01) required cleaning with errors="coerce"

Region names were inconsistent (east, EAST, East)

Visualizations required temporarily disabling multiple plt.show() calls to avoid Tk window blocking

Dataset only includes one month (May), so seasonal variation is limited

ğŸ“ How to Reproduce

From project root:

uv sync
uv run python -m analytics_project.olap_seasonal


Screenshots should be saved to:

docs/images/

ğŸ‰ P6 Complete

This phase provided a full OLAP-style analysis pipeline:

Warehouse â†’ Clean â†’ Join â†’ Cube â†’ OLAP â†’ Visual Storytelling
Ready for real BI decision-making.
